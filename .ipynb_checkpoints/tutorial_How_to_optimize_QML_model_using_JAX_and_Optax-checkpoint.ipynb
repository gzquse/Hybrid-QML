{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:29:25.760559265Z",
     "start_time": "2024-02-07T20:29:25.658500918Z"
    }
   },
   "outputs": [],
   "source": [
    "# This cell is added by sphinx-gallery\n",
    "# It can be customized to whatever you like\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to optimize a QML model using JAX and Optax\n",
    "===============================================\n",
    "\n",
    "Once you have set up a quantum machine learning model, data to train\n",
    "with and cost function to minimize as an objective, the next step is to\n",
    "**perform the optimization**. That is, setting up a classical\n",
    "optimization loop to find a minimal value of your cost function.\n",
    "\n",
    "In this example, we'll show you how to use\n",
    "[JAX](https://jax.readthedocs.io), an autodifferentiable machine\n",
    "learning framework, and [Optax](https://optax.readthedocs.io/), a suite\n",
    "of JAX-compatible gradient-based optimizers, to optimize a PennyLane\n",
    "quantum machine learning model.\n",
    "\n",
    "![](../_static/demonstration_assets/How_to_optimize_QML_model_using_JAX_and_Optax/socialsthumbnail_large_How_to_optimize_QML_model_using_JAX_and_Optax_2024-01-16.png){.align-center\n",
    "width=\"50.0%\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your model, data, and cost\n",
    "=================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will create a simple QML model for our optimization. In\n",
    "particular:\n",
    "\n",
    "-   We will embed our data through a series of rotation gates.\n",
    "-   We will then have an ansatz of trainable rotation gates with\n",
    "    parameters `weights`; it is these values we will train to minimize\n",
    "    our cost function.\n",
    "-   We will train the QML model on `data`, a `(5, 4)` array, and\n",
    "    optimize the model to match target predictions given by `target`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:35:51.739855154Z",
     "start_time": "2024-02-07T20:35:51.674391736Z"
    }
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "\n",
    "n_wires = 5\n",
    "data = jnp.sin(jnp.mgrid[-2:2:0.2].reshape(n_wires, -1)) ** 3\n",
    "targets = jnp.array([-0.2, 0.4, 0.35, 0.2])\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_wires)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(data, weights):\n",
    "    \"\"\"Quantum circuit ansatz\"\"\"\n",
    "\n",
    "    # data embedding\n",
    "    for i in range(n_wires):\n",
    "        # data[i] will be of shape (4,); we are\n",
    "        # taking advantage of operation vectorization here\n",
    "        qml.RY(data[i], wires=i)\n",
    "\n",
    "    # trainable ansatz\n",
    "    for i in range(n_wires):\n",
    "        qml.RX(weights[i, 0], wires=i)\n",
    "        qml.RY(weights[i, 1], wires=i)\n",
    "        qml.RX(weights[i, 2], wires=i)\n",
    "        qml.CNOT(wires=[i, (i + 1) % n_wires])\n",
    "\n",
    "    # we use a sum of local Z's as an observable since a\n",
    "    # local Z would only be affected by params on that qubit.\n",
    "    return qml.expval(qml.sum(*[qml.PauliZ(i) for i in range(n_wires)]))\n",
    "\n",
    "def my_model(data, weights, bias):\n",
    "    return circuit(data, weights) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a simple cost function that computes the overlap between\n",
    "model output and target data, and [just-in-time (JIT)\n",
    "compile](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)\n",
    "it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:35:58.066439521Z",
     "start_time": "2024-02-07T20:35:58.056205867Z"
    }
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(params, data, targets):\n",
    "    predictions = my_model(data, params[\"weights\"], params[\"bias\"])\n",
    "    loss = jnp.sum((targets - predictions) ** 2 / len(data))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model above is just an example for demonstration -- there\n",
    "are important considerations that must be taken into account when\n",
    "performing QML research, including methods for data embedding, circuit\n",
    "architecture, and cost function, in order to build models that may have\n",
    "use. This is still an active area of research; see our\n",
    "[demonstrations](https://pennylane.ai/qml/demonstrations) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize your parameters\n",
    "==========================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can generate our trainable parameters `weights` and `bias` that\n",
    "will be used to train our QML model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:36:00.395186774Z",
     "start_time": "2024-02-07T20:36:00.348144186Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = jnp.ones([n_wires, 3])\n",
    "bias = jnp.array(0.)\n",
    "params = {\"weights\": weights, \"bias\": bias}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging the trainable parameters, data, and target labels into our cost\n",
    "function, we can see the current loss as well as the parameter\n",
    "gradients:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:36:03.972951355Z",
     "start_time": "2024-02-07T20:36:01.698009474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29232618\n",
      "{'bias': Array(-0.754321, dtype=float32, weak_type=True), 'weights': Array([[-1.9507733e-01,  5.2854650e-02, -4.8925212e-01],\n",
      "       [-1.9968867e-02, -5.3287148e-02,  9.2290469e-02],\n",
      "       [-2.7175695e-03, -9.6455216e-05, -4.7958046e-03],\n",
      "       [-6.3544422e-02,  3.6111072e-02, -2.0519713e-01],\n",
      "       [-9.0263695e-02,  1.6375928e-01, -5.6426275e-01]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(loss_fn(params, data, targets))\n",
    "\n",
    "print(jax.grad(loss_fn)(params, data, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the optimizer\n",
    "====================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use Optax to create an optimizer, and train our circuit.\n",
    "Here, we choose the Adam optimizer, however [other available\n",
    "optimizers](https://optax.readthedocs.io/en/latest/api.html) may be used\n",
    "here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:38:28.297788783Z",
     "start_time": "2024-02-07T20:38:28.236626945Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = optax.adam(learning_rate=0.3)\n",
    "opt_state = opt.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define our `update_step` function, which needs to do a couple\n",
    "of things:\n",
    "\n",
    "-   Compute the loss function (so we can track training) and the\n",
    "    gradients (so we can apply an optimization step). We can do this in\n",
    "    one execution via the `jax.value_and_grad` function.\n",
    "-   Apply the update step of our optimizer via `opt.update`\n",
    "-   Update the parameters via `optax.apply_updates`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:38:34.222804028Z",
     "start_time": "2024-02-07T20:38:31.493603559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 0.2923261821269989\n",
      "Step: 5 Loss: 0.04476672783493996\n",
      "Step: 10 Loss: 0.03190239891409874\n",
      "Step: 15 Loss: 0.03623729571700096\n",
      "Step: 20 Loss: 0.03370063751935959\n",
      "Step: 25 Loss: 0.028723960742354393\n",
      "Step: 30 Loss: 0.02301179990172386\n",
      "Step: 35 Loss: 0.018715690821409225\n",
      "Step: 40 Loss: 0.014776434749364853\n",
      "Step: 45 Loss: 0.010427684523165226\n",
      "Step: 50 Loss: 0.009645545855164528\n",
      "Step: 55 Loss: 0.024109993129968643\n",
      "Step: 60 Loss: 0.00808287039399147\n",
      "Step: 65 Loss: 0.00760952103883028\n",
      "Step: 70 Loss: 0.007097803056240082\n",
      "Step: 75 Loss: 0.006783411838114262\n",
      "Step: 80 Loss: 0.006902276072651148\n",
      "Step: 85 Loss: 0.006584083661437035\n",
      "Step: 90 Loss: 0.006034402176737785\n",
      "Step: 95 Loss: 0.0049751754850149155\n"
     ]
    }
   ],
   "source": [
    "def update_step(params, opt_state, data, targets):\n",
    "    loss_val, grads = jax.value_and_grad(loss_fn)(params, data, targets)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_val\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for i in range(100):\n",
    "    params, opt_state, loss_val = update_step(params, opt_state, data, targets)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Step: {i} Loss: {loss_val}\")\n",
    "\n",
    "    loss_history.append(loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jitting the optimization loop\n",
    "=============================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we JIT compiled our cost function `loss_fn`.\n",
    "However, we can also JIT compile the entire optimization loop; this\n",
    "means that the for-loop around optimization is not happening in Python,\n",
    "but is compiled and executed natively. This avoids (potentially costly)\n",
    "data transfer between Python and our JIT compiled cost function with\n",
    "each update step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:29:32.319778603Z",
     "start_time": "2024-02-07T20:29:32.314546746Z"
    }
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_step_jit(i, args):\n",
    "    params, opt_state, data, targets, print_training = args\n",
    "\n",
    "    loss_val, grads = jax.value_and_grad(loss_fn)(params, data, targets)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    def print_fn():\n",
    "        jax.debug.print(\"Step: {i}  Loss: {loss_val}\", i=i, loss_val=loss_val)\n",
    "\n",
    "    # if print_training=True, print the loss every 5 steps\n",
    "    jax.lax.cond((jnp.mod(i, 5) == 0) & print_training, print_fn, lambda: None)\n",
    "\n",
    "    return (params, opt_state, data, targets, print_training)\n",
    "\n",
    "@jax.jit\n",
    "def optimization_jit(params, data, targets, print_training=False):\n",
    "    opt = optax.adam(learning_rate=0.3)\n",
    "    opt_state = opt.init(params)\n",
    "\n",
    "    args = (params, opt_state, data, targets, print_training)\n",
    "    (params, opt_state, _, _, _) = jax.lax.fori_loop(0, 100, update_step_jit, args)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use `jax.lax.fori_loop` and `jax.lax.cond`, rather than a\n",
    "standard Python for loop and if statement, to allow the control flow to\n",
    "be JIT compatible. We also use `jax.debug.print` to allow printing to\n",
    "take place at function run-time, rather than compile-time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:29:33.628343094Z",
     "start_time": "2024-02-07T20:29:32.319071430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  Loss: 0.2923261821269989\n",
      "Step: 5  Loss: 0.044766709208488464\n",
      "Step: 10  Loss: 0.03190242126584053\n",
      "Step: 15  Loss: 0.03623732179403305\n",
      "Step: 20  Loss: 0.03370067477226257\n",
      "Step: 25  Loss: 0.028724024072289467\n",
      "Step: 30  Loss: 0.023011792451143265\n",
      "Step: 35  Loss: 0.018715735524892807\n",
      "Step: 40  Loss: 0.014776429161429405\n",
      "Step: 45  Loss: 0.010427681729197502\n",
      "Step: 50  Loss: 0.009645579382777214\n",
      "Step: 55  Loss: 0.024109533056616783\n",
      "Step: 60  Loss: 0.008082757703959942\n",
      "Step: 65  Loss: 0.0076092444360256195\n",
      "Step: 70  Loss: 0.007097671739757061\n",
      "Step: 75  Loss: 0.006783545948565006\n",
      "Step: 80  Loss: 0.006901954300701618\n",
      "Step: 85  Loss: 0.006584125570952892\n",
      "Step: 90  Loss: 0.006033747456967831\n",
      "Step: 95  Loss: 0.0049752178601920605\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'bias': Array(-0.75290495, dtype=float32),\n 'weights': Array([[ 1.630918  ,  1.5501642 ,  0.6721541 ],\n        [ 0.7266173 ,  0.3642349 , -0.7562605 ],\n        [ 2.7837987 ,  0.62709916,  3.450068  ],\n        [-1.101276  , -0.12706573,  0.89288384],\n        [ 1.2723563 ,  1.1062955 ,  2.2205076 ]], dtype=float32)}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"weights\": weights, \"bias\": bias}\n",
    "optimization_jit(params, data, targets, print_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appendix: Timing the two approaches\n",
    "===================================\n",
    "\n",
    "We can time the two approaches (JIT compiling just the cost function, vs\n",
    "JIT compiling the entire optimization loop) to explore the differences\n",
    "in performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T20:29:42.468354714Z",
     "start_time": "2024-02-07T20:29:33.624548382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jitting just the cost (best of 5): 0.7518415104714222 sec per loop\n",
      "Jitting the entire optimization (best of 5): 0.0035007075057365 sec per loop\n"
     ]
    }
   ],
   "source": [
    "from timeit import repeat\n",
    "\n",
    "def optimization(params, data, targets):\n",
    "    opt = optax.adam(learning_rate=0.3)\n",
    "    opt_state = opt.init(params)\n",
    "\n",
    "    for i in range(100):\n",
    "        params, opt_state, loss_val = update_step(params, opt_state, data, targets)\n",
    "\n",
    "    return params\n",
    "\n",
    "reps = 5\n",
    "num = 2\n",
    "\n",
    "times = repeat(\"optimization(params, data, targets)\", globals=globals(), number=num, repeat=reps)\n",
    "result = min(times) / num\n",
    "\n",
    "print(f\"Jitting just the cost (best of {reps}): {result} sec per loop\")\n",
    "\n",
    "times = repeat(\"optimization_jit(params, data, targets)\", globals=globals(), number=num, repeat=reps)\n",
    "result = min(times) / num\n",
    "\n",
    "print(f\"Jitting the entire optimization (best of {reps}): {result} sec per loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, JIT compiling the entire optimization loop is\n",
    "significantly more performant.\n",
    "\n",
    "About the authors\n",
    "=================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
