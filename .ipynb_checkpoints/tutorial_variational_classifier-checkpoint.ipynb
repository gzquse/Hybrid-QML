{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:25:55.770359788Z",
     "start_time": "2024-02-20T16:25:55.374255109Z"
    }
   },
   "outputs": [],
   "source": [
    "# This cell is added by sphinx-gallery\n",
    "# It can be customized to whatever you like\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational classifier {#variational_classifier}\n",
    "======================\n",
    "\n",
    "::: {.meta}\n",
    ":property=\\\"og:description\\\": Using PennyLane to implement quantum\n",
    "circuits that can be trained from labelled data to classify new data\n",
    "samples. :property=\\\"og:image\\\":\n",
    "<https://pennylane.ai/qml/_static/demonstration_assets//classifier_output_59_0.png>\n",
    ":::\n",
    "\n",
    "::: {.related}\n",
    "tutorial\\_data\\_reuploading\\_classifier Data-reuploading classifier\n",
    "tutorial\\_multiclass\\_classification Multiclass margin classifier\n",
    "ensemble\\_multi\\_qpu Ensemble classification with Rigetti and Qiskit\n",
    "devices\n",
    ":::\n",
    "\n",
    "*Author: Maria Schuld --- Posted: 11 October 2019. Last updated: 11\n",
    "December 2023.*\n",
    "\n",
    "In this tutorial, we show how to use PennyLane to implement variational\n",
    "quantum classifiers - quantum circuits that can be trained from labelled\n",
    "data to classify new data samples. The two examples used are inspired by\n",
    "two of the first papers that proposed variational circuits as supervised\n",
    "machine learning models: [Farhi and Neven\n",
    "(2018)](https://arxiv.org/abs/1802.06002) as well as [Schuld et al.\n",
    "(2018)](https://arxiv.org/abs/1804.00633).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precisely, the first example shows that a variational circuit can\n",
    "be optimized to emulate the parity function\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f: x \\in \\{0,1\\}^{\\otimes n} \\rightarrow y =\n",
    "\\begin{cases} 1 \\text{  if uneven number of 1's in } x \\\\ 0\n",
    "\\text{ else}. \\end{cases}\n",
    "\\end{aligned}$$\n",
    "\n",
    "It demonstrates how to encode binary inputs into the initial state of\n",
    "the variational circuit, which is simply a computational basis state\n",
    "(*basis encoding*).\n",
    "\n",
    "The second example shows how to encode real vectors as amplitude vectors\n",
    "into quantum states (*amplitude encoding*) and how to train a\n",
    "variational circuit to recognize the first two classes of flowers in the\n",
    "Iris dataset.\n",
    "\n",
    "1. Fitting the parity function\n",
    "==============================\n",
    "\n",
    "Imports\n",
    "-------\n",
    "\n",
    "We start by importing PennyLane, the PennyLane-provided version of\n",
    "NumPy, and an optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:25:57.343492091Z",
     "start_time": "2024-02-20T16:25:56.329039728Z"
    }
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantum and classical nodes\n",
    "===========================\n",
    "\n",
    "We then create a quantum device that will run our circuits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:25:57.353185959Z",
     "start_time": "2024-02-20T16:25:57.346486629Z"
    }
   },
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational classifiers usually define a \"layer\" or \"block\", which is an\n",
    "elementary circuit architecture that gets repeated to build the full\n",
    "variational circuit.\n",
    "\n",
    "Our circuit layer will use four qubits, or wires, and consists of an\n",
    "arbitrary rotation on every qubit, as well as a ring of CNOTs that\n",
    "entangles each qubit with its neighbour. Borrowing from machine\n",
    "learning, we call the parameters of the layer `weights`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:25:57.355244978Z",
     "start_time": "2024-02-20T16:25:57.351032656Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer(layer_weights):\n",
    "    for wire in range(4):\n",
    "        qml.Rot(*layer_weights[wire], wires=wire)\n",
    "\n",
    "    for wires in ([0, 1], [1, 2], [2, 3], [3, 0]):\n",
    "        qml.CNOT(wires)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to encode data inputs $x$ into the circuit, so that\n",
    "the measured output depends on the inputs. In this first example, the\n",
    "inputs are bitstrings, which we encode into the state of the qubits. The\n",
    "quantum state $\\psi$ after state preparation is a computational basis\n",
    "state that has 1s where $x$ has 1s, for example\n",
    "\n",
    "$$x = 0101 \\rightarrow |\\psi \\rangle = |0101 \\rangle .$$\n",
    "\n",
    "The `~pennylane.BasisState`{.interpreted-text role=\"class\"} function\n",
    "provided by PennyLane is made to do just this. It expects `x` to be a\n",
    "list of zeros and ones, i.e. `[0,1,0,1]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:25:57.755078092Z",
     "start_time": "2024-02-20T16:25:57.740366718Z"
    }
   },
   "outputs": [],
   "source": [
    "def state_preparation(x):\n",
    "    qml.BasisState(x, wires=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the variational quantum circuit as this state preparation\n",
    "routine, followed by a repetition of the layer structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:25:58.230650294Z",
     "start_time": "2024-02-20T16:25:58.212863155Z"
    }
   },
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit(weights, x):\n",
    "    state_preparation(x)\n",
    "\n",
    "    for layer_weights in weights:\n",
    "        layer(layer_weights)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to add a \"classical\" bias parameter, the variational quantum\n",
    "classifier also needs some post-processing. We define the full model as\n",
    "a sum of the output of the quantum circuit, plus the trainable bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:25:58.566289912Z",
     "start_time": "2024-02-20T16:25:58.550500317Z"
    }
   },
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost\n",
    "====\n",
    "\n",
    "In supervised learning, the cost function is usually the sum of a loss\n",
    "function and a regularizer. We restrict ourselves to the standard square\n",
    "loss that measures the distance between target labels and model\n",
    "predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:25:58.921312294Z",
     "start_time": "2024-02-20T16:25:58.895447529Z"
    }
   },
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    # We use a call to qml.math.stack to allow subtracting the arrays directly\n",
    "    return np.mean((labels - qml.math.stack(predictions)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor how many inputs the current classifier predicted correctly,\n",
    "we also define the accuracy, or the proportion of predictions that agree\n",
    "with a set of target labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:25:59.254575098Z",
     "start_time": "2024-02-20T16:25:59.234217815Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n",
    "    acc = acc / len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning tasks, the cost depends on the data - here the features and\n",
    "labels considered in the iteration of the optimization routine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization\n",
    "============\n",
    "\n",
    "Let's now load and preprocess some data.\n",
    "\n",
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "The parity dataset\\'s\n",
    "`<a href=\"https://raw.githubusercontent.com/XanaduAI/qml/master/_static/demonstration_assets/variational_classifier/data/parity_train.txt\"\n",
    "download=parity.txt target=\"_blank\">train</a>`{.interpreted-text\n",
    "role=\"html\"} and\n",
    "`<a href=\"https://raw.githubusercontent.com/XanaduAI/qml/master/_static/demonstration_assets/variational_classifier/data/parity_test.txt\"\n",
    "download=parity.txt target=\"_blank\">test</a>`{.interpreted-text\n",
    "role=\"html\"} sets can be downloaded and should be placed in the\n",
    "subfolder `variational_classifier/data`.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"variational_classifier/data/parity_train.txt\", dtype=int)\n",
    "X = np.array(data[:, :-1])\n",
    "Y = np.array(data[:, -1])\n",
    "Y = Y * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "for x,y in zip(X, Y):\n",
    "    print(f\"x = {x}, y = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the variables randomly (but fix a seed for\n",
    "reproducibility). Remember that one of the variables is used as a bias,\n",
    "while the rest is fed into the gates of the variational circuit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "print(\"Weights:\", weights_init)\n",
    "print(\"Bias: \", bias_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an optimizer instance and choose a batch size...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(0.5)\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and run the optimizer to train our model. We track the accuracy - the\n",
    "share of correctly classified data samples. For this we compute the\n",
    "outputs of the variational classifier and turn them into predictions in\n",
    "$\\{-1,1\\}$ by taking the sign of the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(100):\n",
    "\n",
    "    # Update the weights by one optimizer step, using only a limited batch of data\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    weights, bias = opt.step(cost, weights, bias, X=X_batch, Y=Y_batch)\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "\n",
    "    current_cost = cost(weights, bias, X, Y)\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the variational classifier learned to classify all bit\n",
    "strings from the training set correctly.\n",
    "\n",
    "But unlike optimization, in machine learning the goal is to generalize\n",
    "from limited data to *unseen* examples. Even if the variational quantum\n",
    "circuit was perfectly optimized with respect to the cost, it might not\n",
    "generalize, a phenomenon known as *overfitting*. The art of (quantum)\n",
    "machine learning is to create models and learning procedures that tend\n",
    "to find \\\"good\\\" minima, or those that lead to models which generalize\n",
    "well.\n",
    "\n",
    "With this in mind, let\\'s look at a test set of examples we have not\n",
    "used during training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"variational_classifier/data/parity_test.txt\", dtype=int)\n",
    "X_test = np.array(data[:, :-1])\n",
    "Y_test = np.array(data[:, -1])\n",
    "Y_test = Y_test * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "    \n",
    "predictions_test = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "\n",
    "for x,y,p in zip(X_test, Y_test, predictions_test):\n",
    "    print(f\"x = {x}, y = {y}, pred={p}\")\n",
    "    \n",
    "acc_test = accuracy(Y_test, predictions_test)\n",
    "print(\"Accuracy on unseen data:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantum circuit has also learnt to predict all unseen examples\n",
    "perfectly well! This is actually remarkable, since the encoding strategy\n",
    "creates quantum states from the data that have zero overlap \\-- and\n",
    "hence the states created from the test set have no overlap with the\n",
    "states created from the training set. There are many functional\n",
    "relations the variational circuit could learn from this kind of\n",
    "representation, but the classifier chooses to label bit strings\n",
    "according to our ground truth, the parity function.\n",
    "\n",
    "Let\\'s look at the second example, in which we use another encoding\n",
    "strategy.\n",
    "\n",
    "2. Iris classification\n",
    "======================\n",
    "\n",
    "We now move on to classifying data points from the Iris dataset, which\n",
    "are no longer simple bitstrings but represented as real-valued vectors.\n",
    "The vectors are 2-dimensional, but we will add some \\\"latent\n",
    "dimensions\\\" and therefore encode inputs into 2 qubits.\n",
    "\n",
    "Quantum and classical nodes\n",
    "---------------------------\n",
    "\n",
    "State preparation is not as simple as when we represent a bitstring with\n",
    "a basis state. Every input x has to be translated into a set of angles\n",
    "which can get fed into a small routine for state preparation. To\n",
    "simplify things a bit, we will work with data from the positive\n",
    "subspace, so that we can ignore signs (which would require another\n",
    "cascade of rotations around the Z-axis).\n",
    "\n",
    "The circuit is coded according to the scheme in [Möttönen, et al.\n",
    "(2004)](https://arxiv.org/abs/quant-ph/0407010), or---as presented for\n",
    "positive vectors only---in [Schuld and Petruccione\n",
    "(2018)](https://link.springer.com/book/10.1007/978-3-319-96424-9). We\n",
    "also decomposed controlled Y-axis rotations into more basic gates,\n",
    "following [Nielsen and Chuang\n",
    "(2010)](http://www.michaelnielsen.org/qcqi/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_angles(x):\n",
    "    beta0 = 2 * np.arcsin(np.sqrt(x[1] ** 2) / np.sqrt(x[0] ** 2 + x[1] ** 2 + 1e-12))\n",
    "    beta1 = 2 * np.arcsin(np.sqrt(x[3] ** 2) / np.sqrt(x[2] ** 2 + x[3] ** 2 + 1e-12))\n",
    "    beta2 = 2 * np.arcsin(np.linalg.norm(x[2:]) / np.linalg.norm(x))\n",
    "\n",
    "    return np.array([beta2, -beta1 / 2, beta1 / 2, -beta0 / 2, beta0 / 2])\n",
    "\n",
    "\n",
    "def state_preparation(a):\n",
    "    qml.RY(a[0], wires=0)\n",
    "\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[1], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[2], wires=1)\n",
    "\n",
    "    qml.PauliX(wires=0)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[3], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[4], wires=1)\n",
    "    qml.PauliX(wires=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if this routine actually works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([0.53896774, 0.79503606, 0.27826503, 0.0], requires_grad=False)\n",
    "ang = get_angles(x)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def test(angles):\n",
    "    state_preparation(angles)\n",
    "\n",
    "    return qml.state()\n",
    "\n",
    "\n",
    "state = test(ang)\n",
    "\n",
    "print(\"x               : \", np.round(x, 6))\n",
    "print(\"angles          : \", np.round(ang, 6))\n",
    "print(\"amplitude vector: \", np.round(np.real(state), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method computed the correct angles to prepare the desired state!\n",
    "\n",
    "> ::: {.note}\n",
    "> ::: {.title}\n",
    "> Note\n",
    "> :::\n",
    ">\n",
    "> The `default.qubit` simulator provides a shortcut to\n",
    "> `state_preparation` with the command `qml.StatePrep(x, wires=[0, 1])`.\n",
    "> On state simulators, this just replaces the quantum state with our\n",
    "> (normalized) input. On hardware, the operation implements more\n",
    "> sophisticated versions of the routine used above.\n",
    "> :::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with only 2 qubits now, we need to update the\n",
    "`layer` function. In addition, we redefine the `cost` function to pass\n",
    "the full batch of data to the state preparation of the circuit\n",
    "simultaneously, a technique similar to NumPy broadcasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def layer(layer_weights):\n",
    "    for wire in range(2):\n",
    "        qml.Rot(*layer_weights[wire], wires=wire)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "\n",
    "\n",
    "def cost(weights, bias, X, Y):\n",
    "    # Transpose the batch of input data in order to make the indexing\n",
    "    # in state_preparation work\n",
    "    predictions = variational_classifier(weights, bias, X.T)\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "We load the Iris data set. There is a bit of preprocessing to do in\n",
    "order to encode the inputs into the amplitudes of a quantum state. We\n",
    "will augment the data points by two so-called \\\"latent dimensions\\\",\n",
    "making the size of the padded data point match the size of the state\n",
    "vector in the quantum device. We then need to normalize the data points,\n",
    "and finally, we translate the inputs x to rotation angles using the\n",
    "`get_angles` function we defined above.\n",
    "\n",
    "Data preprocessing should always be done with the problem in mind; for\n",
    "example, if we do not add any latent dimensions, normalization erases\n",
    "any information on the length of the vectors and classes separated by\n",
    "this feature will not be distinguishable.\n",
    "\n",
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "The Iris dataset can be downloaded\n",
    "`<a href=\"https://raw.githubusercontent.com/XanaduAI/qml/master/_static/demonstration_assets/variational_classifier/data/iris_classes1and2_scaled.txt\"\n",
    "download=parity.txt target=\"_blank\">here</a>`{.interpreted-text\n",
    "role=\"html\"} and should be placed in the subfolder\n",
    "`variational_classifer/data`.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"variational_classifier/data/iris_classes1and2_scaled.txt\")\n",
    "X = data[:, 0:2]\n",
    "print(f\"First X sample (original)  : {X[0]}\")\n",
    "\n",
    "# pad the vectors to size 2^2=4 with constant values\n",
    "padding = np.ones((len(X), 2)) * 0.1\n",
    "X_pad = np.c_[X, padding]\n",
    "print(f\"First X sample (padded)    : {X_pad[0]}\")\n",
    "\n",
    "# normalize each input\n",
    "normalization = np.sqrt(np.sum(X_pad**2, -1))\n",
    "X_norm = (X_pad.T / normalization).T\n",
    "print(f\"First X sample (normalized): {X_norm[0]}\")\n",
    "\n",
    "# the angles for state preparation are the features\n",
    "features = np.array([get_angles(x) for x in X_norm], requires_grad=False)\n",
    "print(f\"First features sample      : {features[0]}\")\n",
    "\n",
    "Y = data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These angles are our new features, which is why we have renamed X to\n",
    "\"features\" above. Let's plot the stages of preprocessing and play around\n",
    "with the dimensions (dim1, dim2). Some of them still separate the\n",
    "classes well, while others are less informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0][Y == 1], X[:, 1][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n",
    "plt.scatter(X[:, 0][Y == -1], X[:, 1][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n",
    "plt.title(\"Original data\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "dim1 = 0\n",
    "dim2 = 1\n",
    "plt.scatter(X_norm[:, dim1][Y == 1], X_norm[:, dim2][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n",
    "plt.scatter(X_norm[:, dim1][Y == -1], X_norm[:, dim2][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n",
    "plt.title(f\"Padded and normalised data (dims {dim1} and {dim2})\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "dim1 = 0\n",
    "dim2 = 3\n",
    "plt.scatter(features[:, dim1][Y == 1], features[:, dim2][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n",
    "plt.scatter(features[:, dim1][Y == -1], features[:, dim2][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n",
    "plt.title(f\"Feature vectors (dims {dim1} and {dim2})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we want to generalize from the data samples. This means that\n",
    "we want to train our model on one set of data and test its performance\n",
    "on a second set of data that has not been used in training. To monitor\n",
    "the generalization performance, the data is split into training and\n",
    "validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_data = len(Y)\n",
    "num_train = int(0.75 * num_data)\n",
    "index = np.random.permutation(range(num_data))\n",
    "feats_train = features[index[:num_train]]\n",
    "Y_train = Y[index[:num_train]]\n",
    "feats_val = features[index[num_train:]]\n",
    "Y_val = Y[index[num_train:]]\n",
    "\n",
    "# We need these later for plotting\n",
    "X_train = X[index[:num_train]]\n",
    "X_val = X[index[num_train:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization\n",
    "============\n",
    "\n",
    "First we initialize the variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_qubits = 2\n",
    "num_layers = 6\n",
    "\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we minimize the cost, using the imported optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(0.01)\n",
    "batch_size = 5\n",
    "\n",
    "# train the variational classifier\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(60):\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "    feats_train_batch = feats_train[batch_index]\n",
    "    Y_train_batch = Y_train[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, feats_train_batch, Y_train_batch)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = np.sign(variational_classifier(weights, bias, feats_train.T))\n",
    "    predictions_val = np.sign(variational_classifier(weights, bias, feats_val.T))\n",
    "\n",
    "    # Compute accuracy on train and validation set\n",
    "    acc_train = accuracy(Y_train, predictions_train)\n",
    "    acc_val = accuracy(Y_val, predictions_val)\n",
    "\n",
    "    if (it + 1) % 2 == 0:\n",
    "        _cost = cost(weights, bias, features, Y)\n",
    "        print(\n",
    "            f\"Iter: {it + 1:5d} | Cost: {_cost:0.7f} | \"\n",
    "            f\"Acc train: {acc_train:0.7f} | Acc validation: {acc_val:0.7f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the continuous output of the variational classifier for the\n",
    "first two dimensions of the Iris data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cm = plt.cm.RdBu\n",
    "\n",
    "# make data for decision regions\n",
    "xx, yy = np.meshgrid(np.linspace(0.0, 1.5, 30), np.linspace(0.0, 1.5, 30))\n",
    "X_grid = [np.array([x, y]) for x, y in zip(xx.flatten(), yy.flatten())]\n",
    "\n",
    "# preprocess grid points like data inputs above\n",
    "padding = 0.1 * np.ones((len(X_grid), 2))\n",
    "X_grid = np.c_[X_grid, padding]  # pad each input\n",
    "normalization = np.sqrt(np.sum(X_grid**2, -1))\n",
    "X_grid = (X_grid.T / normalization).T  # normalize each input\n",
    "features_grid = np.array([get_angles(x) for x in X_grid])  # angles are new features\n",
    "predictions_grid = variational_classifier(weights, bias, features_grid.T)\n",
    "Z = np.reshape(predictions_grid, xx.shape)\n",
    "\n",
    "# plot decision regions\n",
    "levels = np.arange(-1, 1.1, 0.1)\n",
    "cnt = plt.contourf(xx, yy, Z, levels=levels, cmap=cm, alpha=0.8, extend=\"both\")\n",
    "plt.contour(xx, yy, Z, levels=[0.0], colors=(\"black\",), linestyles=(\"--\",), linewidths=(0.8,))\n",
    "plt.colorbar(cnt, ticks=[-1, 0, 1])\n",
    "\n",
    "# plot data\n",
    "for color, label in zip([\"b\", \"r\"], [1, -1]):\n",
    "    plot_x = X_train[:, 0][Y_train == label]\n",
    "    plot_y = X_train[:, 1][Y_train == label]\n",
    "    plt.scatter(plot_x, plot_y, c=color, marker=\"o\", ec=\"k\", label=f\"class {label} train\")\n",
    "    plot_x = (X_val[:, 0][Y_val == label],)\n",
    "    plot_y = (X_val[:, 1][Y_val == label],)\n",
    "    plt.scatter(plot_x, plot_y, c=color, marker=\"^\", ec=\"k\", label=f\"class {label} validation\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the variational classifier learnt a separating line between\n",
    "the datapoints of the two different classes, which allows it to classify\n",
    "even the unseen validation data with perfect accuracy.\n",
    "\n",
    "About the author\n",
    "================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
